{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment & Dictionaries\n",
    "\n",
    "We will mostly be using NLTK to conduct sentiment analysis in this lab\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Corpus\n",
    "\n",
    "NLTK has several corpora. Some are useful for sentiment analysis.\n",
    "\n",
    "http://www.nltk.org/howto/corpus.html\n",
    "\n",
    "* opinion_lexicon\n",
    "* WordNet\n",
    "* SentiWordNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opinion lexicon\n",
    "\n",
    "Opinion Lexicon: A list of English positive and negative opinion words or sentiment words (around 6800 words). This list was compiled over many years starting from in the paper by (Hu and Liu, KDD-2004).\n",
    "\n",
    "You need to first download this nltk opinion_lexicon corpus\n",
    "`nltk.download('opinion_lexicon')`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('opinion_lexicon') #this download needs to happen for the very first time\n",
    "from nltk.corpus import opinion_lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a+', 'abound', 'abounds', 'abundance', 'abundant', ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.positive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_lexicon.positive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2-faced', '2-faces', 'abnormal', 'abolish', ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span class=\"mark\">Your turn</span>**: think of three positive and negative sentiment words. See if they are in the lexicons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with your own words\n",
    "my_pos = ['good','great','groovy']\n",
    "my_neg = ['sick','demented','nasty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD, POS, NEG\n",
      "---------------\n",
      "good True False\n",
      "great True False\n",
      "groovy False False\n",
      "sick False True\n",
      "demented False False\n",
      "nasty False True\n"
     ]
    }
   ],
   "source": [
    "# run this to see if they are in any of the lexicon\n",
    "print('WORD, POS, NEG\\n---------------')\n",
    "for lex in [my_pos,my_neg]:\n",
    "    for word in lex:\n",
    "        print(word,word in opinion_lexicon.positive(),word in opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results tells you that for certain words, opinion_lexicon is not able to assign positive or negative labels. Trying with a non-sentiment word you will see the same result  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of tweet\n",
    "In the last lab, you all tried tokenizing tweets.\n",
    "\n",
    "**<span class=\"mark\">TODO</span>**: What's the sentiment of a tweet sample? \n",
    "You can try with \"@john lol that was #awesome :)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ False False\n",
      "john False False\n",
      "lol False False\n",
      "that False False\n",
      "was False False\n",
      "# False False\n",
      "awesome True False\n",
      ": False False\n",
      ") False False\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "test_tweet = \"@john lol that was #awesome :)\"\n",
    "\n",
    "#your code below\n",
    "tweetoken = nltk.word_tokenize(test_tweet)\n",
    "for word in tweetoken:\n",
    "        print(word,word in opinion_lexicon.positive(),word in opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment analysis with `VADER`\n",
    "https://pypi.org/project/vaderSentiment/\n",
    "\n",
    "VADER Sentiment Analysis. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.\n",
    "\n",
    "`VADER`在发现负面情绪方面表现更好,适合应用于社交媒体文本情感分析\n",
    "\n",
    "返回一个字典，其中包含文本出现肯定pos，否定neg和中立neu的可能性，再过滤筛选出可能性最高的情感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer #pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.872}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "analyzer.polarity_scores(test_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with another text. News article this time. Recall this text from last lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Netanyahu's visit was cut short by reports late Sunday that a rocket was fired from Gaza into central Israel, wounding at least seven people. Following criticism from political opponents over what they consider the prime minister's unclear stance toward the militant political group, Israel responded with a series of strikes into Gaza against Hamas, which largely governs the contested strip. President Donald Trump tacitly endorsed the strike following his meetings with Netanyahu, calling the Hamas attack \\\"despicable.\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.2, 'neu': 0.778, 'pos': 0.023, 'compound': -0.9287}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to interpret the overall score?**\n",
    "\n",
    "The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n",
    "\n",
    "* Positive sentiment: compound score >= 0.05\n",
    "* Neutral sentiment: -0.05 < compound score < 0.05 : \n",
    "* Negative sentiment: compound score <= -0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-dimensional measures of sentiment**\n",
    "\n",
    "The `pos`, `neu`, and `neg` scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span class=\"mark\">TODO</span>**:\n",
    "\n",
    "write function to interpret the overall sentiment of text as positive, negavitve, or neutral based on VADER's analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"news[ 'polarity' ]=news[ 'headline_text' ].map( lambda x: get_vader_score(x))\\npolarity=news[ 'polarity' ].replace({ 0 : 'neg' , 1 : 'neu' , 2 : 'pos' })\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vader_score (sent) :\n",
    "    # Polarity score returns dictionary\n",
    "    ss = sid.polarity_scores(sent)\n",
    "    #return ss\n",
    "    return np.argmax(list(ss.values())[: -1 ])\n",
    "\n",
    "\"\"\"news[ 'polarity' ]=news[ 'headline_text' ].map( lambda x: get_vader_score(x))\n",
    "polarity=news[ 'polarity' ].replace({ 0 : 'neg' , 1 : 'neu' , 2 : 'pos' })\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment analysis with `TextBlob`\n",
    "\n",
    "https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "返回：   \n",
    "    \n",
    "    polarity 极性:[-1,1], 1表示肯定陈述, -1表示否定陈述\n",
    "  \n",
    "    subjectivity 主观性:[0,1], 指个人的意见和感受如何影响某人的判断力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.7666666666666666, subjectivity=0.9)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from texctytblob import TextBlob #pip install TextBlob\n",
    "\n",
    "blob = TextBlob(test_tweet)\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7666666666666666"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.polarity #[-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.subjectivity #[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying with another text. News article this time. Recall this text from last lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Netanyahu's visit was cut short by reports late Sunday that a rocket was fired from Gaza into central Israel, wounding at least seven people. Following criticism from political opponents over what they consider the prime minister's unclear stance toward the militant political group, Israel responded with a series of strikes into Gaza against Hamas, which largely governs the contested strip. President Donald Trump tacitly endorsed the strike following his meetings with Netanyahu, calling the Hamas attack \\\"despicable.\\\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04285714285714285"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(text)\n",
    "blob.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.04285714285714285, subjectivity=0.2642857142857143)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.subjectivity\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few other functions available as well. Press tab to see them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Few more tests to see rule-based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.8, subjectivity=0.75)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('great').sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.4, subjectivity=0.75)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('not great').sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the rule above for \"not great\" is polarity of \"great\" X -0.5 = 0.8* -0.5 = -0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span class=\"mark\">TODO for fun</span>**\n",
    "\n",
    "Try with a few different variations to see whether you can observe the rules working here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Empath`\n",
    "\n",
    "https://github.com/Ejhfast/empath-client\n",
    "\n",
    "https://pypi.org/project/empath/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "移情分析的主要目标是将文本连接到各种不同的情感中,使用相似度比较来映射到Empath的词汇表。Empath值不仅仅是通过计算pos,neg,neu的极性，在计算出现次数的基础上，还对分析出的单词总数进行了归一化。这不仅考虑了计算最频繁的情感并假设它与好坏相关性,更可以深一步挖掘主观感受"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from empath import Empath #pip install empath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon = Empath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ = lexicon.analyze(\"he hit the other person\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories for the sentence: \"he hit the other person\":\n",
      "movement\n",
      "violence\n",
      "pain\n",
      "negative_emotion\n"
     ]
    }
   ],
   "source": [
    "print('Categories for the sentence: \"he hit the other person\":')\n",
    "for key, value in categ.items():\n",
    "    if value != 0:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical'])\n"
     ]
    }
   ],
   "source": [
    "#available categories in empath\n",
    "print(categ.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'help': 0.0,\n",
       " 'office': 0.0,\n",
       " 'dance': 0.0,\n",
       " 'money': 0.0,\n",
       " 'wedding': 0.0,\n",
       " 'domestic_work': 0.0,\n",
       " 'sleep': 0.0,\n",
       " 'medical_emergency': 0.0,\n",
       " 'cold': 0.0,\n",
       " 'hate': 0.0,\n",
       " 'cheerfulness': 0.0,\n",
       " 'aggression': 0.0,\n",
       " 'occupation': 0.0,\n",
       " 'envy': 0.0,\n",
       " 'anticipation': 0.0,\n",
       " 'family': 0.0,\n",
       " 'vacation': 0.0,\n",
       " 'crime': 0.0,\n",
       " 'attractive': 0.0,\n",
       " 'masculine': 0.0,\n",
       " 'prison': 0.0,\n",
       " 'health': 0.0,\n",
       " 'pride': 0.0,\n",
       " 'dispute': 0.0,\n",
       " 'nervousness': 0.0,\n",
       " 'government': 0.0,\n",
       " 'weakness': 0.0,\n",
       " 'horror': 0.0,\n",
       " 'swearing_terms': 0.0,\n",
       " 'leisure': 0.0,\n",
       " 'suffering': 0.0,\n",
       " 'royalty': 0.0,\n",
       " 'wealthy': 0.0,\n",
       " 'tourism': 0.0,\n",
       " 'furniture': 0.0,\n",
       " 'school': 0.0,\n",
       " 'magic': 0.0,\n",
       " 'beach': 0.0,\n",
       " 'journalism': 0.0,\n",
       " 'morning': 0.0,\n",
       " 'banking': 0.0,\n",
       " 'social_media': 0.0,\n",
       " 'exercise': 0.0,\n",
       " 'night': 0.0,\n",
       " 'kill': 0.0,\n",
       " 'blue_collar_job': 0.0,\n",
       " 'art': 0.0,\n",
       " 'ridicule': 0.0,\n",
       " 'play': 0.0,\n",
       " 'computer': 0.0,\n",
       " 'college': 0.0,\n",
       " 'optimism': 0.0,\n",
       " 'stealing': 0.0,\n",
       " 'real_estate': 0.0,\n",
       " 'home': 0.0,\n",
       " 'divine': 0.0,\n",
       " 'sexual': 0.0,\n",
       " 'fear': 0.0,\n",
       " 'irritability': 0.0,\n",
       " 'superhero': 0.0,\n",
       " 'business': 0.0,\n",
       " 'driving': 0.0,\n",
       " 'pet': 0.0,\n",
       " 'childish': 0.0,\n",
       " 'cooking': 0.0,\n",
       " 'exasperation': 0.0,\n",
       " 'religion': 0.0,\n",
       " 'hipster': 0.0,\n",
       " 'internet': 0.0,\n",
       " 'surprise': 0.0,\n",
       " 'reading': 0.0,\n",
       " 'worship': 0.0,\n",
       " 'leader': 0.0,\n",
       " 'independence': 0.0,\n",
       " 'movement': 0.0,\n",
       " 'body': 0.0,\n",
       " 'noise': 0.0,\n",
       " 'eating': 0.0,\n",
       " 'medieval': 0.0,\n",
       " 'zest': 0.0,\n",
       " 'confusion': 0.0,\n",
       " 'water': 0.0,\n",
       " 'sports': 0.0,\n",
       " 'death': 0.0,\n",
       " 'healing': 0.0,\n",
       " 'legend': 0.0,\n",
       " 'heroic': 0.0,\n",
       " 'celebration': 0.0,\n",
       " 'restaurant': 0.0,\n",
       " 'violence': 0.0,\n",
       " 'programming': 0.0,\n",
       " 'dominant_heirarchical': 0.0,\n",
       " 'military': 0.0,\n",
       " 'neglect': 0.0,\n",
       " 'swimming': 0.0,\n",
       " 'exotic': 0.0,\n",
       " 'love': 0.0,\n",
       " 'hiking': 0.0,\n",
       " 'communication': 0.0,\n",
       " 'hearing': 0.0,\n",
       " 'order': 0.0,\n",
       " 'sympathy': 0.0,\n",
       " 'hygiene': 0.0,\n",
       " 'weather': 0.0,\n",
       " 'anonymity': 0.0,\n",
       " 'trust': 0.0,\n",
       " 'ancient': 0.0,\n",
       " 'deception': 0.0,\n",
       " 'fabric': 0.0,\n",
       " 'air_travel': 0.0,\n",
       " 'fight': 0.0,\n",
       " 'dominant_personality': 0.0,\n",
       " 'music': 0.0,\n",
       " 'vehicle': 0.0,\n",
       " 'politeness': 0.0,\n",
       " 'toy': 0.0,\n",
       " 'farming': 0.0,\n",
       " 'meeting': 0.0,\n",
       " 'war': 0.0,\n",
       " 'speaking': 0.0,\n",
       " 'listen': 0.0,\n",
       " 'urban': 0.0,\n",
       " 'shopping': 0.0,\n",
       " 'disgust': 0.0,\n",
       " 'fire': 0.0,\n",
       " 'tool': 0.0,\n",
       " 'phone': 0.0,\n",
       " 'gain': 0.0,\n",
       " 'sound': 0.0,\n",
       " 'injury': 0.0,\n",
       " 'sailing': 0.0,\n",
       " 'rage': 0.0,\n",
       " 'science': 0.0,\n",
       " 'work': 0.0,\n",
       " 'appearance': 0.0,\n",
       " 'valuable': 0.0,\n",
       " 'warmth': 0.0,\n",
       " 'youth': 0.0,\n",
       " 'sadness': 0.0,\n",
       " 'fun': 0.0,\n",
       " 'emotional': 0.0,\n",
       " 'joy': 0.0,\n",
       " 'affection': 0.0,\n",
       " 'traveling': 0.0,\n",
       " 'fashion': 0.0,\n",
       " 'ugliness': 0.0,\n",
       " 'lust': 0.0,\n",
       " 'shame': 0.0,\n",
       " 'torment': 0.0,\n",
       " 'economics': 0.0,\n",
       " 'anger': 0.0,\n",
       " 'politics': 0.0,\n",
       " 'ship': 0.0,\n",
       " 'clothing': 0.0,\n",
       " 'car': 0.0,\n",
       " 'strength': 0.0,\n",
       " 'technology': 0.0,\n",
       " 'breaking': 0.0,\n",
       " 'shape_and_size': 0.0,\n",
       " 'power': 0.0,\n",
       " 'white_collar_job': 0.0,\n",
       " 'animal': 0.0,\n",
       " 'party': 0.0,\n",
       " 'terrorism': 0.0,\n",
       " 'smell': 0.0,\n",
       " 'disappointment': 0.0,\n",
       " 'poor': 0.0,\n",
       " 'plant': 0.0,\n",
       " 'pain': 0.0,\n",
       " 'beauty': 0.0,\n",
       " 'timidity': 0.0,\n",
       " 'philosophy': 0.0,\n",
       " 'negotiate': 0.0,\n",
       " 'negative_emotion': 0.0,\n",
       " 'cleaning': 0.0,\n",
       " 'messaging': 0.0,\n",
       " 'competing': 0.0,\n",
       " 'law': 0.0,\n",
       " 'friends': 0.0,\n",
       " 'payment': 0.0,\n",
       " 'achievement': 0.0,\n",
       " 'alcohol': 0.0,\n",
       " 'liquid': 0.0,\n",
       " 'feminine': 0.0,\n",
       " 'weapon': 0.0,\n",
       " 'children': 0.0,\n",
       " 'monster': 0.0,\n",
       " 'ocean': 0.0,\n",
       " 'giving': 0.0,\n",
       " 'contentment': 0.0,\n",
       " 'writing': 0.0,\n",
       " 'rural': 0.0,\n",
       " 'positive_emotion': 0.0,\n",
       " 'musical': 0.0}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's see how Empath works on our tweet text\n",
    "categ_tweets = lexicon.analyze(test_tweet)\n",
    "categ_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories for the sentence: @john lol that was #awesome :)\n"
     ]
    }
   ],
   "source": [
    "print('Categories for the sentence:', test_tweet)\n",
    "for key, value in categ_tweets.items():\n",
    "    if value != 0:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'help': 0.0,\n",
       " 'office': 0.0,\n",
       " 'dance': 0.0,\n",
       " 'money': 0.0,\n",
       " 'wedding': 0.0,\n",
       " 'domestic_work': 0.0,\n",
       " 'sleep': 0.0,\n",
       " 'medical_emergency': 0.0,\n",
       " 'cold': 0.0,\n",
       " 'hate': 0.0,\n",
       " 'cheerfulness': 0.0,\n",
       " 'aggression': 1.0,\n",
       " 'occupation': 0.0,\n",
       " 'envy': 0.0,\n",
       " 'anticipation': 0.0,\n",
       " 'family': 1.0,\n",
       " 'vacation': 1.0,\n",
       " 'crime': 0.0,\n",
       " 'attractive': 0.0,\n",
       " 'masculine': 0.0,\n",
       " 'prison': 0.0,\n",
       " 'health': 0.0,\n",
       " 'pride': 0.0,\n",
       " 'dispute': 0.0,\n",
       " 'nervousness': 0.0,\n",
       " 'government': 0.0,\n",
       " 'weakness': 0.0,\n",
       " 'horror': 0.0,\n",
       " 'swearing_terms': 0.0,\n",
       " 'leisure': 0.0,\n",
       " 'suffering': 0.0,\n",
       " 'royalty': 0.0,\n",
       " 'wealthy': 0.0,\n",
       " 'tourism': 1.0,\n",
       " 'furniture': 0.0,\n",
       " 'school': 0.0,\n",
       " 'magic': 0.0,\n",
       " 'beach': 0.0,\n",
       " 'journalism': 0.0,\n",
       " 'morning': 0.0,\n",
       " 'banking': 0.0,\n",
       " 'social_media': 1.0,\n",
       " 'exercise': 0.0,\n",
       " 'night': 0.0,\n",
       " 'kill': 1.0,\n",
       " 'blue_collar_job': 0.0,\n",
       " 'art': 0.0,\n",
       " 'ridicule': 0.0,\n",
       " 'play': 0.0,\n",
       " 'computer': 0.0,\n",
       " 'college': 0.0,\n",
       " 'optimism': 0.0,\n",
       " 'stealing': 0.0,\n",
       " 'real_estate': 0.0,\n",
       " 'home': 0.0,\n",
       " 'divine': 0.0,\n",
       " 'sexual': 0.0,\n",
       " 'fear': 0.0,\n",
       " 'irritability': 0.0,\n",
       " 'superhero': 0.0,\n",
       " 'business': 0.0,\n",
       " 'driving': 0.0,\n",
       " 'pet': 0.0,\n",
       " 'childish': 0.0,\n",
       " 'cooking': 0.0,\n",
       " 'exasperation': 0.0,\n",
       " 'religion': 0.0,\n",
       " 'hipster': 0.0,\n",
       " 'internet': 0.0,\n",
       " 'surprise': 0.0,\n",
       " 'reading': 1.0,\n",
       " 'worship': 0.0,\n",
       " 'leader': 0.0,\n",
       " 'independence': 0.0,\n",
       " 'movement': 0.0,\n",
       " 'body': 0.0,\n",
       " 'noise': 0.0,\n",
       " 'eating': 0.0,\n",
       " 'medieval': 0.0,\n",
       " 'zest': 0.0,\n",
       " 'confusion': 0.0,\n",
       " 'water': 0.0,\n",
       " 'sports': 0.0,\n",
       " 'death': 0.0,\n",
       " 'healing': 0.0,\n",
       " 'legend': 0.0,\n",
       " 'heroic': 0.0,\n",
       " 'celebration': 0.0,\n",
       " 'restaurant': 0.0,\n",
       " 'violence': 1.0,\n",
       " 'programming': 0.0,\n",
       " 'dominant_heirarchical': 0.0,\n",
       " 'military': 0.0,\n",
       " 'neglect': 0.0,\n",
       " 'swimming': 0.0,\n",
       " 'exotic': 0.0,\n",
       " 'love': 0.0,\n",
       " 'hiking': 0.0,\n",
       " 'communication': 1.0,\n",
       " 'hearing': 0.0,\n",
       " 'order': 0.0,\n",
       " 'sympathy': 0.0,\n",
       " 'hygiene': 0.0,\n",
       " 'weather': 0.0,\n",
       " 'anonymity': 0.0,\n",
       " 'trust': 0.0,\n",
       " 'ancient': 0.0,\n",
       " 'deception': 1.0,\n",
       " 'fabric': 0.0,\n",
       " 'air_travel': 0.0,\n",
       " 'fight': 1.0,\n",
       " 'dominant_personality': 0.0,\n",
       " 'music': 0.0,\n",
       " 'vehicle': 0.0,\n",
       " 'politeness': 0.0,\n",
       " 'toy': 0.0,\n",
       " 'farming': 0.0,\n",
       " 'meeting': 1.0,\n",
       " 'war': 1.0,\n",
       " 'speaking': 0.0,\n",
       " 'listen': 0.0,\n",
       " 'urban': 1.0,\n",
       " 'shopping': 0.0,\n",
       " 'disgust': 0.0,\n",
       " 'fire': 0.0,\n",
       " 'tool': 0.0,\n",
       " 'phone': 1.0,\n",
       " 'gain': 0.0,\n",
       " 'sound': 0.0,\n",
       " 'injury': 1.0,\n",
       " 'sailing': 0.0,\n",
       " 'rage': 0.0,\n",
       " 'science': 0.0,\n",
       " 'work': 0.0,\n",
       " 'appearance': 1.0,\n",
       " 'valuable': 0.0,\n",
       " 'warmth': 0.0,\n",
       " 'youth': 0.0,\n",
       " 'sadness': 0.0,\n",
       " 'fun': 0.0,\n",
       " 'emotional': 0.0,\n",
       " 'joy': 0.0,\n",
       " 'affection': 0.0,\n",
       " 'traveling': 1.0,\n",
       " 'fashion': 0.0,\n",
       " 'ugliness': 0.0,\n",
       " 'lust': 0.0,\n",
       " 'shame': 0.0,\n",
       " 'torment': 0.0,\n",
       " 'economics': 0.0,\n",
       " 'anger': 0.0,\n",
       " 'politics': 0.0,\n",
       " 'ship': 1.0,\n",
       " 'clothing': 0.0,\n",
       " 'car': 0.0,\n",
       " 'strength': 0.0,\n",
       " 'technology': 0.0,\n",
       " 'breaking': 1.0,\n",
       " 'shape_and_size': 0.0,\n",
       " 'power': 0.0,\n",
       " 'white_collar_job': 0.0,\n",
       " 'animal': 0.0,\n",
       " 'party': 0.0,\n",
       " 'terrorism': 0.0,\n",
       " 'smell': 0.0,\n",
       " 'disappointment': 0.0,\n",
       " 'poor': 0.0,\n",
       " 'plant': 0.0,\n",
       " 'pain': 0.0,\n",
       " 'beauty': 0.0,\n",
       " 'timidity': 0.0,\n",
       " 'philosophy': 0.0,\n",
       " 'negotiate': 0.0,\n",
       " 'negative_emotion': 0.0,\n",
       " 'cleaning': 0.0,\n",
       " 'messaging': 0.0,\n",
       " 'competing': 0.0,\n",
       " 'law': 0.0,\n",
       " 'friends': 1.0,\n",
       " 'payment': 0.0,\n",
       " 'achievement': 0.0,\n",
       " 'alcohol': 0.0,\n",
       " 'liquid': 0.0,\n",
       " 'feminine': 0.0,\n",
       " 'weapon': 2.0,\n",
       " 'children': 0.0,\n",
       " 'monster': 0.0,\n",
       " 'ocean': 0.0,\n",
       " 'giving': 0.0,\n",
       " 'contentment': 0.0,\n",
       " 'writing': 0.0,\n",
       " 'rural': 0.0,\n",
       " 'positive_emotion': 0.0,\n",
       " 'musical': 0.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categ_text = lexicon.analyze(text)\n",
    "categ_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories for the news sentence: Netanyahu's visit was cut short by reports late Sunday that a rocket was fired from Gaza into central Israel, wounding at least seven people. Following criticism from political opponents over what they consider the prime minister's unclear stance toward the militant political group, Israel responded with a series of strikes into Gaza against Hamas, which largely governs the contested strip. President Donald Trump tacitly endorsed the strike following his meetings with Netanyahu, calling the Hamas attack \"despicable.\" \n",
      "---------\n",
      "aggression\n",
      "family\n",
      "vacation\n",
      "tourism\n",
      "social_media\n",
      "kill\n",
      "reading\n",
      "violence\n",
      "communication\n",
      "deception\n",
      "fight\n",
      "meeting\n",
      "war\n",
      "urban\n",
      "phone\n",
      "injury\n",
      "appearance\n",
      "traveling\n",
      "ship\n",
      "breaking\n",
      "friends\n",
      "weapon\n"
     ]
    }
   ],
   "source": [
    "print('Categories for the news sentence:', text, '\\n---------')\n",
    "for key, value in categ_text.items():\n",
    "    if value != 0:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<span class=\"mark\">TODO</span>**: \n",
    "\n",
    "1. From the project pitches that you all submitted, you had some idea of what data to collect. Get one data point for your problem (this could be one reddit post from a community, one tweet, etc.)\n",
    "2. Now check to see which categories of Empath are present\n",
    "3. Now loop through your entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadKeys(key_file):\n",
    "    with open(key_file) as f:\n",
    "        key_dict = json.load(f)\n",
    "    return key_dict['api_key'], key_dict['api_secret'], key_dict['token'], key_dict['token_secret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'twitterkeys-test.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-d69f41f0e912>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mKEY_FILE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'twitterkeys-test.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_secret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_secret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKEY_FILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOAuthHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_access_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_secret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mapi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-cd8b7aa89119>\u001b[0m in \u001b[0;36mloadKeys\u001b[0;34m(key_file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloadKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mkey_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mkey_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_key'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'api_secret'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'token_secret'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'twitterkeys-test.json'"
     ]
    }
   ],
   "source": [
    "KEY_FILE = 'twitterkeys-test.json'\n",
    "api_key, api_secret, token, token_secret = loadKeys(KEY_FILE)\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(token, token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-16594adfe97e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mno_of_pages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_search\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_of_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\033[1mtweet :\\033[0m \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "search_term = \"COVID19\"\n",
    "new_search = search_term + \" -filter:retweets\"\n",
    "no_of_pages = 1\n",
    "\n",
    "for page in tweepy.Cursor(api.search, q = new_search, lang=\"en\",).pages(no_of_pages):\n",
    "    for status in page:\n",
    "        print(\"\\033[1mtweet :\\033[0m \" + status.text)\n",
    "        categ_text = lexicon.analyze(status.text)\n",
    "        for key, value in categ_text.items():\n",
    "            if value != 0:\n",
    "                print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
